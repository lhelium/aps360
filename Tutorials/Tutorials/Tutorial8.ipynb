{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial8.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-ucpaGQWVMS6"},"source":["## Reinforcement Learning - Frozen Lake Sample Code\n","\n","![alt text](https://miro.medium.com/max/842/1*Qp14HWQfOeE2UoSxrxCxAg.png)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GTC5-EAVYeQP"},"source":["#### Frozen Lake Simulation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZIhkR9UHVFU5","colab":{}},"source":["# Frozen Lake Simulation\n","import numpy as np\n","import numpy.random as rnd\n","\n","class Frozen_Lake_Sim:\n","  # Simulation of frozen lake \n","\n","  def __init__(self):\n","    # frozen lake constructor\n","    self.loc = (0,0) #current location\n","    self.state = 0 #current state\n","    self.actions_st = ['d','u','r','l'] #possible actions as str\n","    self.actions_rm = {'d':'u', 'u':'d', 'l':'r', 'r':'l'} #for removing opposite action\n","    self.actions_xy = {'d':(1,0), 'u':(-1,0), 'l':(0,-1), 'r':(0,1)} #action movement\n","\n","    # map of lake\n","    self.lake =   np.array([['F', 'F', 'F', 'F'],\n","                            ['F', 'H', 'F', 'H'],\n","                            ['F', 'F', 'F', 'H'],\n","                            ['H', 'F', 'F', 'G']])\n","  \n","  def state(self):\n","    # retrive current state\n","    return self.state\n","\n","  def retrieve(self, state, action):\n","    # retrive possible actions and states\n","\n","    #possible actions\n","    possible_actions = list(self.actions_st)\n","    possible_actions.remove(self.actions_rm[action]) #removes opposite action\n","\n","    #store information related to three possible actions\n","    T = np.zeros(3, dtype = np.float32)\n","    R = np.zeros(3, dtype = np.int32)\n","    r = np.zeros(3, dtype = np.int32)\n","    sp = np.ones(3, dtype = np.int32)*state\n","\n","    #ensure only valid states are retrieved\n","    if state == 5 or state == 7 or state == 11 or state == 12:\n","      return T, R, sp, possible_action, r\n","\n","    #obtain loc from state\n","    loc = (state // 4, state % 4)\n","\n","    #retrive information for actions\n","    for ii in range(len(possible_actions)):\n","      a_st = possible_actions[ii]\n","      a_xy = self.actions_xy[a_st]\n","      y = loc[0] + a_xy[0]\n","      x = loc[1] + a_xy[1]\n","      \n","      #stay within the boundaries\n","      if not(x<0 or x>3 or y<0 or y>3):\n","        state = (y*4) + x\n","        sp[ii] = state\n","\n","      #determine reward and if we reached end state\n","      reset, reward = 0, 0\n","      if state == 5 or state == 7 or state == 11 or state == 12:\n","        reset = 1\n","      elif state == 15:\n","        reward = 1\n","        reset = 1\n","\n","      #update transition probabilities, reward and end state status  \n","      T[ii] = 0.33\n","      R[ii] = reward\n","      r[ii] = reset\n","\n","    return T, R, sp, possible_actions, r\n","\n","  def move(self, action):\n","    # make movement according to model\n","    \n","    #obtain possible actions from current state\n","    T, R, sp, possible_actions, r = self.retrieve(self.state, action)\n","    \n","    #select from possible actions (transition probability = 0.33)\n","    ii = rnd.choice(list(range(3)))\n","    \n","    #update state and location based on action made\n","    self.state = sp[ii]\n","    self.loc = (self.state // 4, self.state % 4)\n","\n","    return self.state, possible_actions[ii], R[ii], r[ii]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FXJchoGqckJV"},"source":["#### Test Simulator"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"c4Lty0micxtO"},"source":["##### 1. Retrieval"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3QmJ8A09caOQ","outputId":"ad3e8589-90ff-4b12-d4e8-3b0add5d347d","executionInfo":{"status":"ok","timestamp":1587574435407,"user_tz":240,"elapsed":618,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-7ot31psTGtAXzKlWBL6OEtJ7v4FOjLQdRdeG=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["FLS = Frozen_Lake_Sim()\n","#current state\n","print('current state:', FLS.state)\n","\n","#retrive possible actions/end_states from state 14 if we tried to go up\n","T, R, sp, possible_actions, r = FLS.retrieve(14, 'u')\n","for ii in range(3):\n","  print(' action:', possible_actions[ii], 'end_state:', sp[ii], ' T:', T[ii], ' R:', R[ii], ' reset:', r[ii])\n","\n","#current state IS NOT modified by retrieve method\n","print('current state:', FLS.state)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["current state: 0\n"," action: u end_state: 10  T: 0.33  R: 0  reset: 0\n"," action: r end_state: 15  T: 0.33  R: 1  reset: 1\n"," action: l end_state: 13  T: 0.33  R: 0  reset: 0\n","current state: 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AxbHsdUJczd2"},"source":["##### 2. Movement"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xlCUoHO2c4_F","outputId":"26a20716-8d59-4d4a-f6e1-5d519a0e4077","executionInfo":{"status":"ok","timestamp":1587574435408,"user_tz":240,"elapsed":612,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-7ot31psTGtAXzKlWBL6OEtJ7v4FOjLQdRdeG=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["#current state\n","print('current state:', FLS.state)\n","\n","#move\n","end_state, action, R, r = FLS.move('r')\n","print(' action:', action, 'end_state:', end_state, ' R:', R, ' reset:', r)\n","#current state IS modified by move method\n","print('current state:', FLS.state)\n","\n","#move is random\n","end_state, action, R, r = FLS.move('r')\n","print(' action:', action, 'end_state:', end_state, ' R:', R, ' reset:', r)\n","#current state IS modified by move method\n","print('current state:', FLS.state)\n","\n","#move is random\n","end_state, action, R, r = FLS.move('r')\n","print(' action:', action, 'end_state:', end_state, ' R:', R, ' reset:', r)\n","#current state IS modified by move method\n","print('current state:', FLS.state)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["current state: 0\n"," action: d end_state: 4  R: 0  reset: 0\n","current state: 4\n"," action: d end_state: 8  R: 0  reset: 0\n","current state: 8\n"," action: d end_state: 12  R: 0  reset: 1\n","current state: 12\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8zDNIyK6iVJi"},"source":["#### Value Iteration Algorithm"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Kwb1lpOhiYUX","colab":{}},"source":["# value iteration sample code\n","FLS = Frozen_Lake_Sim()\n","\n","#initialize Q matrix\n","Q = np.array(16*[4*[0]], dtype = np.float64)\n","\n","#actions and states\n","actions = ['d','u','r','l']\n","states = [0,1,2,3,4,6,8,9,10,13,14] #excluding end states\n","\n","discount_rate = 0.90\n","n_iterations = 2000\n","\n","#apply value iteration algorithm\n","for iteration in range(n_iterations):\n","  Q_prev = Q.copy()\n","\n","  for s in states:\n","    for a in range(4):\n","      #retrieve possible actions\n","      T, R, sp, _, _ = FLS.retrieve(s, actions[a])\n","      Q_temp = np.zeros(3, dtype = np.float64)\n","      #sum over possible actions\n","      for xx in range(len(T)):\n","        Q_temp[xx] = T[xx] * (R[xx] + discount_rate * np.max(Q_prev[sp[xx]]))\n","      Q[s, a] = np.sum(Q_temp)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"umjysWepjgy9"},"source":["#### Visualize Q Matrix"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ae_iN36Ujnky","outputId":"0cc2bc60-d93c-4abb-a4c9-b23b589796f4","executionInfo":{"status":"ok","timestamp":1587574448304,"user_tz":240,"elapsed":322,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-7ot31psTGtAXzKlWBL6OEtJ7v4FOjLQdRdeG=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["print(Q) # states x actions"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.05685286 0.05046533 0.05685286 0.05860123]\n"," [0.03705826 0.05271445 0.03530989 0.03306076]\n"," [0.06089757 0.0496871  0.06489507 0.06617405]\n"," [0.0340309  0.04840812 0.02875442 0.0340309 ]\n"," [0.06270351 0.04119666 0.05631597 0.08010807]\n"," [0.         0.         0.         0.        ]\n"," [0.08426605 0.01965369 0.10391974 0.10391974]\n"," [0.         0.         0.         0.        ]\n"," [0.10722274 0.13101483 0.09210343 0.06270351]\n"," [0.23000447 0.12317745 0.19109306 0.14573842]\n"," [0.25285988 0.09917549 0.21541272 0.28372405]\n"," [0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.29137557 0.25285988 0.3596869  0.17513834]\n"," [0.62137558 0.52109307 0.59881461 0.37564161]\n"," [0.         0.         0.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jzuopvETkkdg"},"source":["#### Obtain Value Function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IC-3WGJpkol_","outputId":"2d73a1e5-77d8-4506-d41f-abbdf7a108ea","executionInfo":{"status":"ok","timestamp":1587574458038,"user_tz":240,"elapsed":310,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-7ot31psTGtAXzKlWBL6OEtJ7v4FOjLQdRdeG=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["print(np.max(Q,axis = 1).reshape(4,4)) # value for each state\n","print(FLS.lake)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.05860123 0.05271445 0.06617405 0.04840812]\n"," [0.08010807 0.         0.10391974 0.        ]\n"," [0.13101483 0.23000447 0.28372405 0.        ]\n"," [0.         0.3596869  0.62137558 0.        ]]\n","[['F' 'F' 'F' 'F']\n"," ['F' 'H' 'F' 'H']\n"," ['F' 'F' 'F' 'H']\n"," ['H' 'F' 'F' 'G']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DFtt3RJxjn6b"},"source":["#### Obtain Policy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iFz_Pv9njpzT","outputId":"3d424d53-b154-4a9f-8be5-99b4f4e04081","colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["print(np.argmax(Q, axis = 1).reshape(4,4)) # optimal action for each state\n","print(FLS.lake)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[3 1 3 1]\n"," [3 0 2 0]\n"," [1 0 3 0]\n"," [0 2 0 0]]\n","[['F' 'F' 'F' 'F']\n"," ['F' 'H' 'F' 'H']\n"," ['F' 'F' 'F' 'H']\n"," ['H' 'F' 'F' 'G']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aQsYxYK0kr9c"},"source":["#### Q - Learning"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bfRFVpjLkroU","colab":{}},"source":["# q-learning sample code\n","FLS = Frozen_Lake_Sim()\n","\n","#initialize Q matrix\n","Q = np.array(16*[4*[0]], dtype = np.float64)\n","\n","#states and actions\n","actions = ['d','u','r','l']\n","states = [0,1,2,3,4,6,8,9,10,13,14]\n","reset = 0\n","\n","learning_rate = 0.05\n","n_iterations_1 = 200000\n","n_iterations_2 = 200\n","discount_rate = 0.9\n","\n","#implement q-learning algorithm\n","for iteration_1 in range(n_iterations_1):\n","  #randomly select starting state\n","  s = rnd.choice(states)\n","  FLS.state = s\n","  for iteration_2 in range(n_iterations_2):\n","    #restart when we reach a hole or goal state\n","    if reset == 1:\n","      reset = 0\n","      break\n","    \n","    #randomly choose action\n","    a = rnd.choice([0,1,2,3])\n","    #make movement in simulation\n","    sp, a_actual, reward, reset = FLS.move(actions[a])\n","    \n","    term1 = (1-learning_rate) * Q[s, a]\n","    term2 = (learning_rate) * (reward + discount_rate * np.max(Q[sp]))\n","    Q[s, a] = term1 + term2\n","    s = sp # move to next state"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Aq02xhl7nBoJ"},"source":["#### Visualize Q Matrix"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mXaht6kGnBoM","outputId":"46e6876d-7170-42e2-fdb9-3806f4c6f922","colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["print(Q) # states x actions"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.1874649  0.17757906 0.18334424 0.19323277]\n"," [0.11104646 0.16953547 0.09817925 0.11144727]\n"," [0.16025747 0.15511725 0.16030573 0.17108729]\n"," [0.08410912 0.14963376 0.08195661 0.1050186 ]\n"," [0.17955983 0.13696377 0.13025654 0.21687732]\n"," [0.         0.         0.         0.        ]\n"," [0.13827286 0.05554225 0.16720656 0.18895715]\n"," [0.         0.         0.         0.        ]\n"," [0.23900484 0.28235982 0.18131458 0.14960372]\n"," [0.35755987 0.19981365 0.21159772 0.29712601]\n"," [0.34402453 0.20246355 0.28466481 0.33238822]\n"," [0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.3560656  0.31717908 0.49944287 0.26260102]\n"," [0.70196326 0.57906805 0.66890483 0.48841097]\n"," [0.         0.         0.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C-h15C1OnBoU"},"source":["#### Obtain Value Function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qxZPa1V_nBoU","outputId":"f1eca658-1b6c-48d9-8e27-d8b34466d63f","colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["print(np.max(Q,axis = 1).reshape(4,4)) # value for each state\n","print(FLS.lake)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.19323277 0.16953547 0.17108729 0.14963376]\n"," [0.21687732 0.         0.18895715 0.        ]\n"," [0.28235982 0.35755987 0.34402453 0.        ]\n"," [0.         0.49944287 0.70196326 0.        ]]\n","[['F' 'F' 'F' 'F']\n"," ['F' 'H' 'F' 'H']\n"," ['F' 'F' 'F' 'H']\n"," ['H' 'F' 'F' 'G']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qyyYjpoKnBoX"},"source":["#### Obtain Policy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dlP8ufKhnBoY","outputId":"86d89eb8-b85d-4824-8d77-e379e72f722b","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["print(np.argmax(Q, axis = 1).reshape(4,4)) # optimal action for each state\n","print(FLS.lake)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[3 1 3 1]\n"," [3 0 3 0]\n"," [1 0 0 0]\n"," [0 2 0 0]]\n","[['F' 'F' 'F' 'F']\n"," ['F' 'H' 'F' 'H']\n"," ['F' 'F' 'F' 'H']\n"," ['H' 'F' 'F' 'G']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qPSpyQqIzAc6","colab_type":"text"},"source":["#### Thing to try\n","- assign -1 reward to holes\n","- implement epsilon greedy\n","- adjust learning rates"]},{"cell_type":"code","metadata":{"id":"vpYfGFymzAc7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}