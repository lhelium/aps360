{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab3.ipynb","provenance":[{"file_id":"1XuRpWggcOUWjFgBBK30HIsl128Yn0xIM","timestamp":1580526920756}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X6WDvajSqIDs"},"source":["# Lab 3: Gesture Recognition using Convolutional Neural Networks\n","\n","**Deadlines**: \n","\n","- Lab 3 Part A: February 6, 11:59pm\n","- Lab 3 Part B: February 13, 11:59pm\n","\n","**Late Penalty**: There is a penalty-free grace period of one hour past the deadline. Any work that is submitted between 1 hour and 24 hours past the deadline will receive a 20% grade deduction. No other late work is accepted. Quercus submission time will be used, not your local computer time. You can submit your labs as many times as you want before the deadline, so please submit often and early.\n","\n","**Grading TAs**: \n","- Lab 3 Part A: Geoff Donoghue  \n","- Lab 3 Part B: Tianshi Cao\n","\n","This lab is based on an assignment developed by Prof. Lisa Zhang.\n","\n","This lab will be completed in two parts. In Part A you will you will gain experience gathering your own data set (specifically images of hand gestures), and understand the challenges involved in the data cleaning process. In Part B you will train a convolutional neural network to make classifications on different hand gestures. By the end of the lab, you should be able to:\n","\n","1. Generate and preprocess your own data\n","2. Load and split data for training, validation and testing\n","3. Train a Convolutional Neural Network\n","4. Apply transfer learning to improve your model\n","\n","Note that for this lab we will not be providing you with any starter code. You should be able to take the code used in previous labs, tutorials and lectures and modify it accordingly to complete the tasks outlined below.\n","\n","### What to submit\n","\n","**Submission for Part A:**  \n","Submit a zip file containing your images. Three images each of American Sign Language gestures for letters A - I (total of 27 images). You will be required to clean the images before submitting them. Details are provided under Part A of the handout.\n","\n","Individual image file names should follow the convention of student-number_Alphabet_file-number.jpg\n","(e.g. 100343434_A_1.jpg).\n","\n","\n","**Submission for Part B:**  \n","Submit a PDF file containing all your code, outputs, and write-up\n","from parts 1-5. You can produce a PDF of your Google Colab file by\n","going to **File > Print** and then save as PDF. The Colab instructions\n","has more information. Make sure to review the PDF submission to ensure that your answers are easy to read. Make sure that your text is not cut off at the margins. \n","\n","**Do not submit any other files produced by your code.**\n","\n","Include a link to your colab file in your submission.\n","\n","Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LfiFE_WOqIDu"},"source":["## Colab Link\n","\n","Include a link to your colab file here\n","\n","Colab Link: https://colab.research.google.com/drive/1VBpsPpVqrz4mYDW-Xnb1zKeOcccyAT21"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kvTXpH_kqIDy"},"source":["## Part A. Data Collection [10 pt]\n","\n","So far, we have worked with data sets that have been collected, cleaned, and curated by machine learning\n","researchers and practitioners. Datasets like MNIST and CIFAR are often used as toy examples, both by\n","students and by researchers testing new machine learning models.\n","\n","In the real world, getting a clean data set is never that easy. More than half the work in applying machine\n","learning is finding, gathering, cleaning, and formatting your data set.\n","\n","The purpose of this lab is to help you gain experience gathering your own data set, and understand the\n","challenges involved in the data cleaning process.\n","\n","### American Sign Language\n","\n","American Sign Language (ASL) is a complete, complex language that employs signs made by moving the\n","hands combined with facial expressions and postures of the body. It is the primary language of many\n","North Americans who are deaf and is one of several communication options used by people who are deaf or\n","hard-of-hearing.\n","\n","The hand gestures representing English alphabet are shown below. This lab focuses on classifying a subset\n","of these hand gesture images using convolutional neural networks. Specifically, given an image of a hand\n","showing one of the letters A-I, we want to detect which letter is being represented.\n","\n","![alt text](https://www.disabled-world.com/pics/1/asl-alphabet.jpg)\n","\n","\n","### Generating Data\n","We will produce the images required for this lab by ourselves. Each student will collect, clean and submit\n","three images each of Americal Sign Language gestures for letters A - I (total of 27 images)\n","Steps involved in data collection\n","\n","1. Familiarize yourself with American Sign Language gestures for letters from A - I (9 letters).\n","2. Ask your friend to take three pictures at slightly different orientation for each letter gesture using your\n","mobile phone.\n"," - Ensure adequate lighting while you are capturing the images.\n"," - Use a white wall as your background.\n"," - Use your right hand to create gestures (for consistency).\n"," - Keep your right hand fairly apart from your body and any other obstructions.\n"," - Avoid having shadows on parts of your hand.\n","3. Transfer the images to your laptop for cleaning.\n","\n","### Cleaning Data\n","To simplify the machine learning the task, we will standardize the training images. We will make sure that\n","all our images are of the same size (224 x 224 pixels RGB), and have the hand in the center of the cropped\n","regions.\n","\n","You may use the following applications to crop and resize your images:\n","\n","**Mac**\n","- Use Preview:\n","– Holding down CMD + Shift will keep a square aspect ratio while selecting the hand area.\n","– Resize to 224x224 pixels.\n","\n","**Windows 10**\n","- Use Photos app to edit and crop the image and keep the aspect ratio a square.\n","- Use Paint to resize the image to the final image size of 224x224 pixels.\n","\n","**Linux**\n","- You can use GIMP, imagemagick, or other tools of your choosing.\n","You may also use online tools such as http://picresize.com\n","All the above steps are illustrative only. You need not follow these steps but following these will ensure that\n","you produce a good quality dataset. You will be judged based on the quality of the images alone.\n","Please do not edit your photos in any other way. You should not need to change the aspect ratio of your\n","image. You also should not digitally remove the background or shadows—instead, take photos with a white\n","background and minimal shadows.\n","\n","### Accepted Images\n","Images will be accepted and graded based on the criteria below\n","1. The final image should be size 224x224 pixels (RGB).\n","2. The file format should be a .jpg file.\n","3. The hand should be approximately centered on the frame.\n","4. The hand should not be obscured or cut off.\n","5. The photos follows the ASL gestures posted earlier.\n","6. The photos were not edited in any other way (e.g. no electronic removal of shadows or background).\n","\n","### Submission\n","Submit a zip file containing your images. There should be a total of 27 images (3 for each category)\n","1. Individual image file names should follow the convention of student-number_Alphabet_file-number.jpg\n","(e.g. 100343434_A_1.jpg)\n","2. Zip all the images together and name it with the following convention: last-name_student-number.zip\n","(e.g. last-name_100343434.zip).\n","3. Submit the zipped folder.\n","We will be anonymizing and combining the images that everyone submits. We will announce when the\n","combined data set will be available for download.\n","\n","![alt text](https://github.com/UTNeural/APS360/blob/master/Gesture%20Images.PNG?raw=true)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bJxMgWGNqID2"},"source":["## Part B. Building a CNN [50 pt]\n","\n","For this lab, we are not going to give you any starter code. You will be writing a convolutional neural network\n","from scratch. You are welcome to use any code from previous labs, lectures and tutorials. You should also\n","write your own code.\n","\n","You may use the PyTorch documentation freely. You might also find online tutorials helpful. However, all\n","code that you submit must be your own.\n","\n","Make sure that your code is vectorized, and does not contain obvious inefficiencies (for example, unecessary\n","for loops, or unnecessary calls to unsqueeze()). Ensure enough comments are included in the code so that\n","your TA can understand what you are doing. It is your responsibility to show that you understand what you\n","write.\n","\n","**This is much more challenging and time-consuming than the previous labs.** Make sure that you\n","give yourself plenty of time by starting early. In particular, the earlier questions can be completed even if you\n","do not yet have the full data set."]},{"cell_type":"code","metadata":{"id":"1x6hZocag0Hr","colab_type":"code","colab":{}},"source":["import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms as transforms\n","\n","import os\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MiDuQaAh56sT"},"source":["### 1. Data Loading and Splitting [10 pt]\n","\n","Download the anonymized data provided from Quercus. Split the data into training, validation,\n","and test sets.\n","\n","Note: Data splitting is not as trivial in this lab. We want our test set to closely resemble the setting in which\n","our model will be used. In particular, our test set should contain hands that are never seen in training!\n","\n","Explain how you split the data, either by describing what you did, or by showing the code that you used.\n","Justify your choice of splitting strategy. How many training, validation, and test images do you have?\n","\n","For loading the data, you can use plt.imread as in Lab 1, or any other method that you choose. You may find\n","torchvision.datasets.ImageFolder helpful. (see https://pytorch.org/docs/master/torchvision/datasets.html#imagefolder\n",") "]},{"cell_type":"code","metadata":{"id":"kWh3yuTUrZEE","colab_type":"code","outputId":"88adae27-a01b-4a27-b13e-ce54132cd32f","executionInfo":{"status":"ok","timestamp":1581367252179,"user_tz":300,"elapsed":25771,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWr-rr9NLIvNZQHMsflqb5HJDbRtfW3LRxJUc8=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":120}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WBrH5kBqRLa6","colab":{}},"source":["# define the training, validation, and testing directories\n","train_path = \"/content/drive/My Drive/Colab Notebooks/APS360/Lab3/Lab_3b_Gesture_Dataset/train\"\n","validation_path = \"/content/drive/My Drive/Colab Notebooks/APS360/Lab3/Lab_3b_Gesture_Dataset/validation\"\n","test_path = \"/content/drive/My Drive/Colab Notebooks/APS360/Lab3/Lab_3b_Gesture_Dataset/test\"\n","\n","# the ASL letters produced in the dataset\n","classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E-j_tZhiXy_t","colab_type":"code","outputId":"8bdada0c-a758-4498-ac76-cf7e07de19ef","executionInfo":{"status":"ok","timestamp":1581367258166,"user_tz":300,"elapsed":31721,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWr-rr9NLIvNZQHMsflqb5HJDbRtfW3LRxJUc8=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["# convert all jpgs to tensors\n","data_transform = transforms.Compose([transforms.Resize((224,224)), \n","                                transforms.ToTensor()])\n","\n","# load training, validation, and testing data\n","train_data = torchvision.datasets.ImageFolder(root = train_path, \n","                                           transform=data_transform)\n","\n","validation_data = torchvision.datasets.ImageFolder(root = validation_path, \n","                                           transform=data_transform)\n","\n","test_data = torchvision.datasets.ImageFolder(root = test_path, \n","                                           transform=data_transform)\n","\n","# check that I'm actually importing things and not just writing bs\n","print('Number of training images:', len(train_data))\n","print('Number of validation images:', len(validation_data))\n","print('Number of testing images:', len(test_data))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Number of training images: 1398\n","Number of validation images: 526\n","Number of testing images: 507\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VBmSeMXVYViz","colab_type":"code","colab":{}},"source":["# define dataloader parameters\n","batch_size  = 32\n","num_workers = 1\n","\n","# prepare data loaders\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AhTl_Uy8Ymik","colab_type":"code","outputId":"23fa9031-e34c-4b95-dfc9-fb78d3abda98","executionInfo":{"status":"ok","timestamp":1581367258170,"user_tz":300,"elapsed":31689,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWr-rr9NLIvNZQHMsflqb5HJDbRtfW3LRxJUc8=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["\"\"\"\n","# check to see if I'm actually importing the images correctly or if stuff's wack asf\n","\n","# obtain one batch of training images\n","dataiter = iter(train_loader)\n","images, labels = dataiter.next()\n","images = images.numpy() # convert images to numpy for display\n","\n","# plot the images in the batch, along with the corresponding labels\n","fig = plt.figure(figsize=(25, 4))\n","for idx in np.arange(20):\n","    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n","    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n","    ax.set_title(classes[labels[idx]])\n","\"\"\""],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# check to see if I'm actually importing the images correctly or if stuff's wack asf\\n\\n# obtain one batch of training images\\ndataiter = iter(train_loader)\\nimages, labels = dataiter.next()\\nimages = images.numpy() # convert images to numpy for display\\n\\n# plot the images in the batch, along with the corresponding labels\\nfig = plt.figure(figsize=(25, 4))\\nfor idx in np.arange(20):\\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\\n    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\\n    ax.set_title(classes[labels[idx]])\\n\""]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"GSUacaEN9WgU","colab_type":"code","outputId":"4c131c50-399d-4f1e-800a-ae7e808ef7ec","executionInfo":{"status":"ok","timestamp":1581367258172,"user_tz":300,"elapsed":31667,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWr-rr9NLIvNZQHMsflqb5HJDbRtfW3LRxJUc8=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["\"\"\"\n","- in Lab 2, all 12,000 images were divided into 2/3 training data, 1/6 validation data, and 1/6 testing data \n","- in this Lab, all letters except for 'I', have either 272 or 273 samples\n","    - 'I' has 249 samples\n","- we need to split the data so that each letter has enough training, validation, and testing data\n","- use ~60% of the images for training, ~20% for validation, ~20% for testing\n","\n","There are 156 training photos, 59 validation photos, and 57 testing photos for A, B, C, E\n","There are 156 training photos, 60 validation photos, and 56 testing photos for D, F, G, H\n","There are 147 training photos, 51 validation photos, and 51 testing photos for I\n","\n","- when the images are split, keep images belonging to the same student together in each dataset\n","    - i.e.: put all three of student 1's 'A' images in the training dataset, put all three of student 2's 'A' images in the\n","    validation dataset, put all three of student 3's 'A' images in the testing dataset\n","- this allows us to test the model on unseen samples (unseen hands)\n","- this is also why the percentages aren't exactly 60%, 20%, and 20%\n","\"\"\""],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n- in Lab 2, all 12,000 images were divided into 2/3 training data, 1/6 validation data, and 1/6 testing data \\n- in this Lab, all letters except for 'I', have either 272 or 273 samples\\n    - 'I' has 249 samples\\n- we need to split the data so that each letter has enough training, validation, and testing data\\n- use ~60% of the images for training, ~20% for validation, ~20% for testing\\n\\nThere are 156 training photos, 59 validation photos, and 57 testing photos for A, B, C, E\\nThere are 156 training photos, 60 validation photos, and 56 testing photos for D, F, G, H\\nThere are 147 training photos, 51 validation photos, and 51 testing photos for I\\n\\n- when the images are split, keep images belonging to the same student together in each dataset\\n    - i.e.: put all three of student 1's 'A' images in the training dataset, put all three of student 2's 'A' images in the\\n    validation dataset, put all three of student 3's 'A' images in the testing dataset\\n- this allows us to test the model on unseen samples (unseen hands)\\n- this is also why the percentages aren't exactly 60%, 20%, and 20%\\n\""]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"yNXejBrDciV5","colab_type":"code","colab":{}},"source":["# some helper functions from Lab 2\n","def get_model_name(name, batch_size, learning_rate, epoch):\n","    ''' Generate a name for the model consisting of all the hyperparameter values\n","\n","    Args:\n","        config: Configuration object containing the hyperparameters\n","    Returns:\n","        path: A string with the hyperparameter name and value concatenated\n","    '''\n","    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n","                                                   batch_size,\n","                                                   learning_rate,\n","                                                   epoch)\n","    return path\n","\n","def get_accuracy(model, train=False):\n","    if train:\n","        data_loader = train_loader\n","    else:\n","        data_loader = val_loader\n","\n","    correct = 0\n","    total = 0\n","    for imgs, labels in data_loader:\n","        \n","        \"\"\"\n","        imgs = alexNet.features(imgs) #SLOW\n","        #############################################\n","        #To Enable GPU Usage\n","        if use_cuda and torch.cuda.is_available():\n","          imgs = imgs.cuda()\n","          labels = labels.cuda()\n","        #############################################\n","        \"\"\"\n","        \n","        output = model(imgs)\n","        \n","        #select index with maximum prediction score\n","        pred = output.max(1, keepdim=True)[1]\n","        correct += pred.eq(labels.view_as(pred)).sum().item()\n","        total += imgs.shape[0]\n","    return correct / total"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MAZ8p9ZsUiuq","colab_type":"text"},"source":["### 2. Model Building and Sanity Checking [15 pt]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5VWX4DGY5gQE"},"source":["### Part (a) Convolutional Network - 5 pt\n","\n","Build a convolutional neural network model that takes the (224x224 RGB) image as input, and predicts the gesture\n","letter. Your model should be a subclass of nn.Module. Explain your choice of neural network architecture: how\n","many layers did you choose? What types of layers did you use? Were they fully-connected or convolutional?\n","What about other decisions like pooling layers, activation functions, number of channels / hidden units?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2dtx1z5951fS","colab":{}},"source":["# from Lab 2\n","\"\"\"class Lab3CNN(nn.Module):\n","    def __init__(self):\n","        super(Lab3CNN, self).__init__()\n","        self.name = \"Lab3CNN\"\n","        self.conv = nn.Conv2d(3, 5, 3)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        # how is the input to the fc determined? use that formula n(i+1) = [n(i) + 2*padding - kernel] / stride \n","        # ok but using that formula I get that the fully connected layer should be 111*111*5???\n","        # unless there are 2 feature maps being used but how would you know???\n","        self.fc = nn.Linear(220 * 220 * 10, 1) # uhh\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv(x)))\n","        print(\"pool relu:\", x.shape)\n","        x = self.pool(x)\n","        print(\"pool:\", x.shape)\n","        x = x.view(-1, 220 * 220 * 10) # uhh why does this change every time\n","        print(\"view:\", x.shape)\n","        x = self.fc(x)\n","        print(\"fc:\", x.shape)\n","        #x = x.squeeze(1) # Flatten to [batch_size]\n","        print(\"squeeze:\", x.shape)\n","        return x\"\"\"\n","\n","class Lab3CNN(nn.Module):\n","    def __init__(self):\n","        super(Lab3CNN, self).__init__()\n","        self.name = \"small\"\n","        self.conv = nn.Conv2d(3, 5, 3)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc = nn.Linear(136125, 9)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv(x)))\n","        x = self.pool(x)\n","        x = x.view(-1, 136125)\n","        x = self.fc(x)\n","        #x = x.squeeze(1) # Flatten to [batch_size]\n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfNz0r6TT4wS","colab_type":"code","outputId":"a5d2220a-dd6c-4702-b5f3-a9f5aa392ef7","executionInfo":{"status":"ok","timestamp":1581367258175,"user_tz":300,"elapsed":31634,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWr-rr9NLIvNZQHMsflqb5HJDbRtfW3LRxJUc8=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["\"\"\"\n","Number of layers: \n","- \n","\n","Types of layers: 1 convolution layer, 1 pooling layer, 1 fully-connected layer\n","- \n","\n","Fully connected or convolutional: convolutional first, then fully-connected\n","- \n","\n","Pooling layers: 1\n","- \n","\n","Activation function: ReLU \n","- \n","\n","Number of hidden units: \n","- \n","\n","\"\"\""],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nNumber of layers: \\n- \\n\\nTypes of layers: 1 convolution layer, 1 pooling layer, 1 fully-connected layer\\n- \\n\\nFully connected or convolutional: convolutional first, then fully-connected\\n- \\n\\nPooling layers: 1\\n- \\n\\nActivation function: ReLU \\n- \\n\\nNumber of hidden units: \\n- \\n\\n'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XeGvelvb515e"},"source":["### Part (b) Training Code - 5 pt\n","\n","Write code that trains your neural network given some training data. Your training code should make it easy\n","to tweak the usual hyperparameters, like batch size, learning rate, and the model object itself. Make sure\n","that you are checkpointing your models from time to time (the frequency is up to you). Explain your choice\n","of loss function and optimizer."]},{"cell_type":"code","metadata":{"id":"732DIaS4IF8p","colab_type":"code","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"43dbfa7b-d38d-4799-febd-e70d15f81a40","executionInfo":{"status":"ok","timestamp":1581367258176,"user_tz":300,"elapsed":31627,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWr-rr9NLIvNZQHMsflqb5HJDbRtfW3LRxJUc8=s64","userId":"16176446408353667842"}}},"source":["#@title \n","\"\"\"def train(model, batch_size=9, learning_rate = 0.01, num_epochs=30):\n","    ########################################################################\n","    # Train a classifier on A-I\n","    target_classes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"]\n","    ########################################################################\n","    # Fixed PyTorch random seed for reproducible result\n","    torch.manual_seed(1000)\n","    ########################################################################\n","    # Obtain the PyTorch data loader objects to load batches of the datasets\n","    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n","    val_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n","    ########################################################################\n","    # Define the Loss function and optimizer\n","    # The loss function will be Binary Cross Entropy (BCE). In this case we\n","    # will use the BCEWithLogitsLoss which takes unnormalized output from\n","    # the neural network and scalar label.\n","    # Optimizer will be SGD with Momentum.\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)    \n","    ########################################################################\n","    # Set up some numpy arrays to store the training/test loss/erruracy\n","    train_err = np.zeros(num_epochs)\n","    train_loss = np.zeros(num_epochs)\n","    val_err = np.zeros(num_epochs)\n","    val_loss = np.zeros(num_epochs)\n","    ########################################################################\n","    # Train the network\n","    # Loop over the data iterator and sample a new batch of training data\n","    # Get the output from the network, and optimize our loss function.\n","    start_time = time.time()\n","    for epoch in range(num_epochs):  # loop over the dataset multiple times\n","        total_train_loss = 0.0\n","        total_train_err = 0.0\n","        total_epoch = 0\n","        for i, data in enumerate(train_loader, 0):\n","            # Get the inputs\n","            inputs, labels = data\n","            #print(\"inputs:\",inputs.shape)\n","            #print(\"labels:\",labels.shape)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = model(inputs).unsqueeze(dim=1)\n","            #print(\"outputs:\",outputs.shape)\n","            #print(\"labels version 2:\", labels.shape)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Calculate the statistics\n","            corr = (outputs > 0.0).squeeze().long() != labels\n","            total_train_err += int(corr.sum())\n","            total_train_loss += loss.item()\n","            total_epoch += len(labels)\n","        train_err[epoch] = float(total_train_err) / total_epoch\n","        train_loss[epoch] = float(total_train_loss) / (i+1)\n","        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n","        print((\"Epoch {}: Train err: {}, Train loss: {} |\"+\n","               \"Validation err: {}, Validation loss: {}\").format(\n","                   epoch + 1,\n","                   train_err[epoch],\n","                   train_loss[epoch],\n","                   val_err[epoch],\n","                   val_loss[epoch]))\n","        # Save the current model (checkpoint) to a file\n","        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n","        torch.save(net.state_dict(), model_path)\n","    print('Finished Training')\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n","    # Write the train/test loss/err into CSV file for plotting later\n","    epochs = np.arange(1, num_epochs + 1)\n","    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n","    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n","    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n","    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\"\"\""],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'def train(model, batch_size=9, learning_rate = 0.01, num_epochs=30):\\n    ########################################################################\\n    # Train a classifier on A-I\\n    target_classes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"]\\n    ########################################################################\\n    # Fixed PyTorch random seed for reproducible result\\n    torch.manual_seed(1000)\\n    ########################################################################\\n    # Obtain the PyTorch data loader objects to load batches of the datasets\\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\\n    val_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\\n    ########################################################################\\n    # Define the Loss function and optimizer\\n    # The loss function will be Binary Cross Entropy (BCE). In this case we\\n    # will use the BCEWithLogitsLoss which takes unnormalized output from\\n    # the neural network and scalar label.\\n    # Optimizer will be SGD with Momentum.\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)    \\n    ########################################################################\\n    # Set up some numpy arrays to store the training/test loss/erruracy\\n    train_err = np.zeros(num_epochs)\\n    train_loss = np.zeros(num_epochs)\\n    val_err = np.zeros(num_epochs)\\n    val_loss = np.zeros(num_epochs)\\n    ########################################################################\\n    # Train the network\\n    # Loop over the data iterator and sample a new batch of training data\\n    # Get the output from the network, and optimize our loss function.\\n    start_time = time.time()\\n    for epoch in range(num_epochs):  # loop over the dataset multiple times\\n        total_train_loss = 0.0\\n        total_train_err = 0.0\\n        total_epoch = 0\\n        for i, data in enumerate(train_loader, 0):\\n            # Get the inputs\\n            inputs, labels = data\\n            #print(\"inputs:\",inputs.shape)\\n            #print(\"labels:\",labels.shape)\\n\\n            # zero the parameter gradients\\n            optimizer.zero_grad()\\n\\n            # forward + backward + optimize\\n            outputs = model(inputs).unsqueeze(dim=1)\\n            #print(\"outputs:\",outputs.shape)\\n            #print(\"labels version 2:\", labels.shape)\\n            loss = criterion(outputs, labels)\\n            loss.backward()\\n            optimizer.step()\\n\\n            # Calculate the statistics\\n            corr = (outputs > 0.0).squeeze().long() != labels\\n            total_train_err += int(corr.sum())\\n            total_train_loss += loss.item()\\n            total_epoch += len(labels)\\n        train_err[epoch] = float(total_train_err) / total_epoch\\n        train_loss[epoch] = float(total_train_loss) / (i+1)\\n        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\\n        print((\"Epoch {}: Train err: {}, Train loss: {} |\"+\\n               \"Validation err: {}, Validation loss: {}\").format(\\n                   epoch + 1,\\n                   train_err[epoch],\\n                   train_loss[epoch],\\n                   val_err[epoch],\\n                   val_loss[epoch]))\\n        # Save the current model (checkpoint) to a file\\n        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\\n        torch.save(net.state_dict(), model_path)\\n    print(\\'Finished Training\\')\\n    end_time = time.time()\\n    elapsed_time = end_time - start_time\\n    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\\n    # Write the train/test loss/err into CSV file for plotting later\\n    epochs = np.arange(1, num_epochs + 1)\\n    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\\n    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\\n    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\\n    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"lt9e6erNbTYm","colab_type":"code","outputId":"0d730dd9-8ad9-41ab-96a6-505fdc12fe2d","executionInfo":{"status":"ok","timestamp":1581368083582,"user_tz":300,"elapsed":476,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWr-rr9NLIvNZQHMsflqb5HJDbRtfW3LRxJUc8=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["def train(model, data, batch_size=9, learning_rate = 0.01, num_epochs=30):\n","    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","\n","    iters, losses, train_acc, val_acc = [], [], [], []\n","\n","    # training\n","    n = 0 # the number of iterations\n","    for epoch in range(num_epochs):\n","        total_train_loss = 0.0\n","        total_train_err = 0.0\n","        total_epoch = 0\n","        for i, data in enumerate(train_loader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data\n","            print(\"\\ninputs:\",inputs.shape)\n","            #print(\"labels:\",labels.shape)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = model(inputs)#.unsqueeze(dim=1)\n","        \n","            print(\"outputs:\",outputs.shape)\n","            print(\"labels:\", labels.shape)\n","            loss = criterion(outputs, labels.long())\n","            loss.backward()\n","            optimizer.step()\n","\"\"\"\n","            # Calculate the statistics\n","            corr = (outputs > 0.0).squeeze().long() != labels\n","            total_train_err += int(corr.sum())\n","            total_train_loss += loss.item()\n","            total_epoch += 1\n","        train_err[epoch] = float(total_train_err) / total_epoch\n","        train_loss[epoch] = float(total_train_loss) / (i+1)\n","        #val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n","        print((\"Epoch {}: Train err: {}, Train loss: {} |\").format(\n","                   epoch + 1,\n","                   train_err[epoch],\n","                   train_loss[epoch]))\n","\n","        model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n","\n","        torch.save(net.state_dict(), model_path)\n","    print('Finished Training')\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n","    # Write the train/test loss/err into CSV file for plotting later\n","    epochs = np.arange(1, num_epochs + 1)\n","    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n","    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n","    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n","    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\"\"\""],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n            # Calculate the statistics\\n            corr = (outputs > 0.0).squeeze().long() != labels\\n            total_train_err += int(corr.sum())\\n            total_train_loss += loss.item()\\n            total_epoch += 1\\n        train_err[epoch] = float(total_train_err) / total_epoch\\n        train_loss[epoch] = float(total_train_loss) / (i+1)\\n        #val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\\n        print((\"Epoch {}: Train err: {}, Train loss: {} |\").format(\\n                   epoch + 1,\\n                   train_err[epoch],\\n                   train_loss[epoch]))\\n\\n        model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\\n\\n        torch.save(net.state_dict(), model_path)\\n    print(\\'Finished Training\\')\\n    end_time = time.time()\\n    elapsed_time = end_time - start_time\\n    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\\n    # Write the train/test loss/err into CSV file for plotting later\\n    epochs = np.arange(1, num_epochs + 1)\\n    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\\n    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\\n    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\\n    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)'"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"tIKAZ1-Db7ep","colab_type":"code","outputId":"dac77154-13a6-4dbb-968f-f6781e9666f5","executionInfo":{"status":"error","timestamp":1581368579194,"user_tz":300,"elapsed":495,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWr-rr9NLIvNZQHMsflqb5HJDbRtfW3LRxJUc8=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":437}},"source":["model = Lab3CNN()\n","train(model, train_data)\n","# ERROR: Target 1 is out of bounds.\n","#model_path_small = get_model_name(\"small\", 64, 0.01, 29)\n","#plot_training_curve(model_path_small)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["\n","inputs: torch.Size([9, 3, 224, 224])\n","outputs: torch.Size([1, 9])\n","labels: torch.Size([9])\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-ee83539b049a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLab3CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# ERROR: Target 1 is out of bounds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model_path_small = get_model_name(\"small\", 64, 0.01, 29)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#plot_training_curve(model_path_small)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-36-df50cddfe792>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, batch_size, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1836\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (9)."]}]},{"cell_type":"code","metadata":{"id":"16Su0Sc3U8fK","colab_type":"code","colab":{}},"source":["\"\"\"\n","Type of loss function chosen: cross-entropy\n","- \n","\n","Type of optimizer chosen: single gradient descent\n","- \n","\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bk1RNgAj54rZ"},"source":["### Part (c) “Overfit” to a Small Dataset - 5 pt\n","\n","One way to sanity check our neural network model and training code is to check whether the model is capable\n","of “overfitting” or “memorizing” a small dataset. A properly constructed CNN with correct training code\n","should be able to memorize the answers to a small number of images quickly.\n","\n","Construct a small dataset (e.g. just the images that you have collected). Then show that your model and\n","training code is capable of memorizing the labels of this small data set.\n","\n","With a large batch size (e.g. the entire small dataset) and learning rate that is not too high, You should be\n","able to obtain a 100% training accuracy on that small dataset relatively quickly (within 200 iterations)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lXYRBhQO6d3u","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FdFZg5tlUwNk","colab_type":"text"},"source":["### 3. Hyperparameter Search [10 pt]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nvDLw-Vz6eVS"},"source":["### Part (a) - 1 pt\n","\n","List 3 hyperparameters that you think are most worth tuning. Choose at least one hyperparameter related to\n","the model architecture."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HXRQbgMqR_Qy","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zeD6EzPB6kSW"},"source":["### Part (b) - 6 pt\n","\n","Tune the hyperparameters you listed in Part (a), trying as many values as you need to until you feel satisfied\n","that you are getting a good model. Plot the training curve of at least 4 different hyperparameter settings."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UkvdR-cB6nzm","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"H93iN5_l60BO"},"source":["### Part (c) - 1 pt\n","Choose the best model out of all the ones that you have trained. Justify your choice."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kIhcN7IG6zRO","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QzNA5oup67JO"},"source":["### Part (d) - 2 pt\n","Report the test accuracy of your best model. You should only do this step once and prior to this step you should have only used the training and validation data."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2eJ7AbVl6-ax","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wrem-iXV6_Bz"},"source":["### 4. Transfer Learning [15 pt]\n","For many image classification tasks, it is generally not a good idea to train a very large deep neural network\n","model from scratch due to the enormous compute requirements and lack of sufficient amounts of training\n","data.\n","\n","One of the better options is to try using an existing model that performs a similar task to the one you need\n","to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed **Transfer\n","Learning**. In this assignment, we will use Transfer Learning to extract features from the hand gesture\n","images. Then, train a smaller network to use these features as input and classify the hand gestures.\n","\n","As you have learned from the CNN lecture, convolution layers extract various features from the images which\n","get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal\n","role in establishing Deep Neural Nets as a go-to tool for image classification problems and we will use an\n","ImageNet pre-trained AlexNet model to extract features in this assignment."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rWdQJz4Q7O2F"},"source":["### Part (a) - 5 pt\n","Here is the code to load the AlexNet network, with pretrained weights. When you first run the code, PyTorch\n","will download the pretrained weights from the internet."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BJKcTW9C7TZk","colab":{}},"source":["import torchvision.models\n","alexnet = torchvision.models.alexnet(pretrained=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NQ0GZYaP7VAR"},"source":["The alexnet model is split up into two components: *alexnet.features* and *alexnet.classifier*. The\n","first neural network component, *alexnet.features*, is used to compute convolutional features, which are\n","taken as input in *alexnet.classifier*.\n","\n","The neural network alexnet.features expects an image tensor of shape Nx3x224x224 as input and it will\n","output a tensor of shape Nx256x6x6 . (N = batch size).\n","\n","Compute the AlexNet features for each of your training, validation, and test data. Here is an example code\n","snippet showing how you can compute the AlexNet features for some images (your actual code might be\n","different):"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oX7SjVdB7XAE","outputId":"b3ab0b45-ea93-4af6-a99c-2ac82e04830d","executionInfo":{"status":"error","timestamp":1581223291033,"user_tz":300,"elapsed":120801,"user":{"displayName":"Laura He","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWr-rr9NLIvNZQHMsflqb5HJDbRtfW3LRxJUc8=s64","userId":"16176446408353667842"}},"colab":{"base_uri":"https://localhost:8080/","height":370}},"source":["# img = ... a PyTorch tensor with shape [N,3,224,224] containing hand images ...\n","data = train_data\n","batchSize = 64\n","\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batchSize)\n","for imgs, labels in iter(train_loader):\n","    imgs = features = alexnet.features(imgs) #SLOW\n","features = alexnet.features(img)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-fd85eb145dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malexnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#SLOW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malexnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m    137\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2773\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2775\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2777\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DYcjHg_A7cCM"},"source":["**Save the computed features**. You will be using these features as input to your neural network in Part\n","(b), and you do not want to re-compute the features every time. Instead, run *alexnet.features* once for\n","each image, and save the result."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TBo1BpL373LX","colab":{}},"source":["# Save Features to Folder (assumes code from 1. has been evaluated)\n","\n","import os\n","import torchvision.models\n","alexnet = torchvision.models.alexnet(pretrained=True)\n","\n","# location on Google Drive\n","master_path = '/content/gdrive/My Drive/Colab Notebooks/APS360/Lab3/Lab3_Data/Features'\n","\n","# Prepare Dataloader (requires code from 1.)\n","batch_size = 1 # save 1 file at a time, hence batch_size = 1\n","num_workers = 1\n","data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n","                                           num_workers=num_workers, shuffle=True)\n","\n","classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\n","\n","# save features to folder as tensors\n","n = 0\n","for img, label in data_loader:\n","  features = alexnet.features(img)\n","  features_tensor = torch.from_numpy(features.detach().numpy())\n","\n","  folder_name = master_path + '/' + str(classes[label])\n","  if not os.path.isdir(folder_name):\n","    os.mkdir(folder_name)\n","  torch.save(features_tensor.squeeze(0), folder_name + '/' + str(n) + '.tensor')\n","  n += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OFWvvhFN73qY"},"source":["### Part (b) - 3 pt\n","Build a convolutional neural network model that takes as input these AlexNet features, and makes a\n","prediction. Your model should be a subclass of nn.Module.\n","\n","Explain your choice of neural network architecture: how many layers did you choose? What types of layers\n","did you use: fully-connected or convolutional? What about other decisions like pooling layers, activation\n","functions, number of channels / hidden units in each layer?\n","\n","Here is an example of how your model may be called:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oVTuHUeV78-U","colab":{}},"source":["# features = ... load precomputed alexnet.features(img) ...\n","output = model(features)\n","prob = F.softmax(output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wVAGuURu7-9q"},"source":["### Part (c) - 5 pt\n","Train your new network, including any hyperparameter tuning. Plot and submit the training curve of your\n","best model only.\n","\n","Note: Depending on how you are caching (saving) your AlexNet features, PyTorch might still be tracking\n","updates to the **AlexNet weights**, which we are not tuning. One workaround is to convert your AlexNet\n","feature tensor into a numpy array, and then back into a PyTorch tensor."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JCmiH11x7-q1","colab":{}},"source":["tensor = torch.from_numpy(tensor.detach().numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hQ2tvqJ68Mqb"},"source":["### Part (d) - 2 pt\n","Report the test accuracy of your best model. How does the test accuracy compare to part 4(d)?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yCp_kFSg8Q2T","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}